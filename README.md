Many large language models (LLMs) like OpenAI’s ChatGPT (ChatGPT, 2022) or Claude3 (Claude, 2023) have evolved rapidly. It is quite hard to differentiate between the AI-generated and human-written text, more so when students use twisted prompts to generate similar content like humans. This research aims to analyze the AI and twisted prompt essays’ patterns and develop a model that’s unbiased to the essay generated by the twisted prompts. The experiments yielded very high reliability of the BERT model in differentiating AI-generated from human-written essays, giving out a high accuracy. Twisted prompt essay analysis showed that twisted prompts produce more variable essays, with human-like features in them, whereas non-twisted prompts resulted in more homogeneous and highly standardized outputs. While the average word and sentence lengths might be similar, the range for that of word length stretches much further in twisted prompt essays, indicating increased variation in vocabulary. Some other notable differences found were related to pronouns, spelling mistakes, and passive voice, with twisted prompts leading to more personalized, error-prone, and human-like writing. Essays generated from twisted prompts contain more named entities, indicators of human authorship, and philosophical sentiment, suggesting deeper engagement with complex themes. Therefore, all factors put together, prompt type very significantly impacts the linguistic characteristics, where twisted prompts really enhance the variability and make the essays more human-like in features, which could further help in differentiating AI-generated content from human-written work. The concatenation of TF-IDF, BERT embeddings, and custom features into a combined model, although a little worse in the performance metrics compared to models trained only with the original Kaggle dataset and its extension, demonstrates visible advantages in robustness. In particular, the final model is very good at bringing out subtle distinctions between human and AI-generated content, even when AI essays are close enough to human writing. Various features have been integrated to enable a deep and superficial text analysis, maximizing the generalizability of the model across types of AI essays. 

# Data Pipeline
![ALt text](https://github.com/Matt-Chang/LLM---Detect-AI-Generated-Text/blob/main/flow%20chart.png)

![ALt text](https://github.com/Matt-Chang/LLM---Detect-AI-Generated-Text/blob/main/Picture1.png)
